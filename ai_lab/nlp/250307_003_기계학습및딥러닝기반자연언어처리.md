## 기계 학습 및 딥러닝 기반 자연어 처리 (NLP) 정리 노트

### 1. 기계 학습 기반 자연어 처리

#### 1.1 개요

* 통계적 기법을 활용하여 텍스트 데이터를 분석하고 예측하는 방식입니다.
* 주로 지도 학습, 비지도 학습, 강화 학습 등의 기계 학습 알고리즘이 사용됩니다.

#### 1.2 주요 기법

* **나이브 베이즈 분류 (Naive Bayes Classification):**
    * 텍스트 분류에 널리 사용되는 확률 기반 알고리즘입니다.
    * 단어의 출현 빈도를 기반으로 문서를 분류합니다.
* **서포트 벡터 머신 (Support Vector Machine, SVM):**
    * 텍스트 분류, 감성 분석 등에 사용되는 강력한 알고리즘입니다.
    * 데이터를 고차원 공간에 매핑하여 최적의 분리 초평면을을 찾습니다.
* **의사 결정 트리 (Decision Tree):**
    * 텍스트 분류, 정보 검색 등에 사용되는 트리 기반 알고리즘입니다.
    * 데이터의 특징을 기반으로 의사 결정 규칙을 생성합니다.
* **숨겨진 마르코프 모델 (Hidden Markov Model, HMM):**
    * 품사 태깅, 음성 인식 등에 사용되는 확률 기반 모델입니다.
    * 시간 순서에 따른 데이터의 패턴을 분석합니다.

#### 1.3 특징

* 상대적으로 적은 데이터로도 학습이 가능합니다.
* 모델의 해석이 용이합니다.
* 복잡한 패턴을 학습하는 데 한계가 있을 수 있습니다.

### 2. 딥러닝 기반 자연어 처리

#### 2.1 개요

* 인공 신경망을 사용하여 텍스트 데이터를 처리하고 학습하는 방식입니다.
* 대량의 데이터를 기반으로 복잡한 패턴을 학습할 수 있습니다.

#### 2.2 주요 기법

* **순환 신경망 (Recurrent Neural Network, RNN):**
    * 시퀀스 데이터를 처리하는 데 특화된 신경망입니다.
    * 문장의 맥락을 파악하는 데 유용하지만, 장기 의존성 문제를 가지고 있습니다.
* **장단기 기억 신경망 (Long Short-Term Memory, LSTM):**
    * RNN의 장기 의존성 문제를 해결한 신경망입니다.
    * 긴 문장의 맥락을 효과적으로 파악할 수 있습니다.
* **게이트 순환 유닛 (Gated Recurrent Unit, GRU):**
    * LSTM의 변형으로, 더 간단한 구조를 가지고 있습니다.
    * LSTM과 유사한 성능을 보이면서 학습 속도가 빠릅니다.
* **트랜스포머 (Transformer):**
    * 어텐션 메커니즘을 사용하여 문장의 맥락을 파악하는 신경망입니다.
    * **BERT(Bidirectional Encoder Representations from Transformers)**와 **GPT(Generative Pre-trained Transformer)**같은 모델의 기반이 됩니다.
    * 뛰어난 성능으로 인해 다양한 NLP 과제에서 널리 사용됩니다.

#### 2.3 특징

* 대량의 데이터를 기반으로 복잡한 패턴을 학습할 수 있습니다.
* 높은 성능을 보이지만, 모델의 해석이 어려울 수 있습니다.
* 많은 연산 자원이 필요합니다.

### 3. 기계 학습/딥러닝 NLP의 발전 방향

* 더욱 복잡하고 다양한 언어 현상을 처리하기 위한 모델 개발
* 소량의 데이터로도 학습할 수 있는 모델 개발
* 모델의 해석 가능성을 높이는 연구
* 다양한 분야와의 융합을 통한 응용 분야 확장
