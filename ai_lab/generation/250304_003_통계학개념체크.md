## 최대 가능도 추정법 (MLE, Maximum Likelihood Estimation)

MLE는 주어진 데이터에서 likelihood (어떤 모수 값이 관측된 데이터를 얼마나 잘 설명하는지)를 최대화하는 모수 값을 찾는 방법입니다. 즉, 우리가 가진 데이터를 가장 잘 '설명'하는 통계 모델의 파라미터를 추정하는 방법입니다.  MLE는 통계적 모델링에서 가장 기본적이고 널리 사용되는 방법 중 하나이며, 다양한 분야에서 활용됩니다.

예를 들어, 동전 던지기를 여러 번 했을 때 앞면이 나온 횟수를 가지고 동전의 앞면이 나올 확률을 추정하는 데 사용할 수 있습니다.  이 때, MLE는 우리가 관찰한 앞면이 나온 횟수를 가장 잘 설명하는 동전의 앞면이 나올 확률을 추정합니다.

**MLE의 기본 원리:**

1.  **데이터:**  관찰된 데이터셋  `X = {x1, x2, ..., xn}` 이 주어집니다.
2.  **모델:** 데이터가 특정 확률 분포를 따른다고 가정합니다.  이 확률 분포는 모수 θ에 의해 결정됩니다 (예: 정규 분포의 평균과 분산).
3.  **가능도 함수:**  가능도 함수 `L(θ|X)`는 주어진 모수 θ 하에서 데이터 X가 관찰될 확률을 나타냅니다.  일반적으로, 독립적인 데이터 샘플의 경우, 각 샘플의 확률을 곱하여 계산합니다.  `L(θ|X) = P(x1|θ) * P(x2|θ) * ... * P(xn|θ)`
4.  **최대화:** 가능도 함수 `L(θ|X)`를 최대화하는 모수 θ 값을 찾습니다.  일반적으로, 로그 가능도 함수 `log L(θ|X)`를 최대화하는 것이 더 쉽습니다.

**MLE의 장점:**

*   비교적 계산이 간단하고 구현하기 쉽습니다.
*   점근적 효율성을 가집니다 (데이터가 많아질수록 추정량이 실제 값에 수렴).

**MLE의 단점:**

*   이상치(outlier)에 민감할 수 있습니다.
*   모델이 잘못 설정된 경우 (데이터가 실제와 다른 분포를 따르는 경우) 성능이 저하될 수 있습니다.

### 가능도와 확률의 차이는?

*   **확률(Probability):** "모수 θ가 주어졌을 때" 특정한 사건이 일어날 가능성.  즉, 이미 알고 있는 모델 파라미터를 사용하여 특정 결과가 발생할 가능성을 계산합니다.
    *   예) 동전이 공정할 때(θ = 0.5), "앞면이 7번 나올 확률"을 계산.  `P(X|θ)`

*   **가능도(Likelihood):** "관찰된 데이터를 기반으로 모수 θ가 어떤 값인지 추정". 즉, 관찰된 데이터를 가장 잘 설명하는 모델 파라미터를 찾는 과정입니다.
    *   예) "앞면 7번이 나왔는데, θ 값이 얼마면 가장 가능성이 높을까?" 를 찾는 것.  `L(θ|X)`
    *   가능도 함수 L(θ) 를 최대로 만드는 θ 값을 찾는 것이 MLE(Maximum Likelihood Estimation)!

**쉽게 말하면:**

*   확률 → θ (모수)가 주어졌을 때 데이터(X)를 얻을 확률을 계산 (`P(X|θ)`)
*   가능도 → 데이터(X)를 보고 θ (모수) 값을 추정하는 과정 (`L(θ|X)`)

### 확률밀도함수 (Probability Density Function, PDF) 란?

연속형 확률변수의 특정 구간 내에 존재할 확률을 나타내는 함수입니다.  PDF는 특정 값에서의 확률이 아니라, 특정 구간에서의 확률을 나타냅니다.  PDF를 특정 구간에 대해 적분하면 해당 구간 내에 확률변수가 존재할 확률을 구할 수 있습니다.

### 베이즈 정리 (Bayes' Theorem)

베이즈 정리는 사전 확률과 가능도를 이용하여 사후 확률을 계산하는 방법입니다.  즉, 새로운 데이터가 주어졌을 때 기존의 믿음(사전 확률)을 업데이트하는 데 사용됩니다.

`P(A|B) = [P(B|A) * P(A)] / P(B)`

*   `P(A|B)`: 사후 확률 (posterior probability) - 사건 B가 발생했을 때 사건 A가 발생할 확률
*   `P(B|A)`: 가능도 (likelihood) - 사건 A가 발생했을 때 사건 B가 발생할 확률
*   `P(A)`: 사전 확률 (prior probability) - 사건 A가 발생할 확률 (사건 B가 발생하기 전의 믿음)
*   `P(B)`: 주변 확률 (marginal probability) - 사건 B가 발생할 확률

### 쿨백-라이블러 발산 (Kullback-Leibler Divergence, KL Divergence)

KL 발산은 두 확률 분포 간의 차이를 측정하는 방법입니다.  한 확률 분포가 다른 확률 분포와 얼마나 다른지를 나타내는 척도로 사용됩니다.  KL 발산은 항상 0 이상이며, 두 분포가 동일할 때 0이 됩니다.  KL 발산은 비대칭적입니다 (즉, `KL(P||Q)`와 `KL(Q||P)`는 일반적으로 다릅니다).

### 복원 오차 (Reconstruction Loss)

오토인코더와 같은 모델에서 입력 데이터를 다시 복원했을 때, 복원된 데이터와 원래 데이터 간의 차이를 나타내는 손실 함수입니다.  복원 오차는 모델이 입력 데이터의 중요한 특징을 얼마나 잘 학습했는지를 나타내는 지표로 사용됩니다.  일반적으로 평균 제곱 오차 (Mean Squared Error, MSE)나 교차 엔트로피 (Cross-Entropy) 등이 복원 오차로 사용됩니다.

### 어텐션 (Attention) 이란?

어텐션 메커니즘은 입력 데이터의 각 부분에 대한 중요도를 학습하여, 특정 부분에 집중하도록 하는 기술입니다.  특히, 자연어 처리 분야에서 문장 내의 단어 간의 관계를 파악하거나, 이미지 처리 분야에서 이미지 내의 객체 간의 관계를 파악하는 데 사용됩니다.  어텐션 메커니즘은 모델이 입력 데이터의 중요한 정보에 집중하고, 불필요한 정보를 무시하도록 도와줍니다.
